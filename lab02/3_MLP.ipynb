{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron\n",
    "A Multi-Layer Perceptron (MLP) is a collection of perceptrons connected and organized in layers. The output of one layer of perceptrons is the input of the next layer. Information travels through the network from input to output nodes. There are special configurations in which the information goes back to previous layers thanks to recurrent connections. Recurrent connections are out of the scope of this laboratory. In this notebook you are going to test how the output of a MLP changes with respect to the changes in its weight connections for different activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets, HBox, VBox\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of some activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear\n",
    "$$output = neta$$\n",
    "\n",
    "Sigmoid\n",
    "$$output = \\frac {1}{1 + e^{-neta}}$$\n",
    "\n",
    "Hyperbolic tangent\n",
    "$$output = \\frac {e^{neta} - e^{-neta}}{e^{neta} + e^{-neta}}$$\n",
    "\n",
    "Gaussian\n",
    "$$output = e^{-neta^{2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(neta):\n",
    "    output = neta\n",
    "    return output\n",
    "\n",
    "def sigmoid(neta):\n",
    "    output = 1 / (1 + np.exp(-neta))\n",
    "    return output\n",
    "\n",
    "def htan(neta):\n",
    "    exp = np.exp(neta)\n",
    "    m_exp = np.exp(-neta)\n",
    "    output = (exp - m_exp ) / (exp + m_exp)\n",
    "    return output\n",
    "\n",
    "def gaussian(neta):\n",
    "    output = np.exp(-1 * neta * neta)\n",
    "    return output\n",
    "\n",
    "activation_functions_dict = {'Linear': linear, 'Sigmoid': sigmoid, 'Hyperbolic tangent': htan, 'Gaussian':gaussian}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "            ______________\n",
    "           /              \\\n",
    "x __w_x__ j                l\n",
    "  _______ | f_act(I.W + b) |----- output\n",
    "y   w_y   l                j\n",
    "           \\______________/\n",
    "Where:\n",
    "x = input x\n",
    "y = input y\n",
    "b = bias\n",
    "f_act = activation function\n",
    "I = vector of inputs [x, y]\n",
    "W = vector of weights [w_x, w_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$output = f\\_act(\\sum_{i=0}^{1}{(I_{i} * W_{i}) + b})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(input_values, weights, bias, activation_function):\n",
    "    neta = np.dot(input_values, weights) + bias\n",
    "    output = activation_function(neta)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                 _________________\n",
    "                /                 \\\n",
    "x _____w_x_0___j                   l   w_h_0\n",
    "     \\        _| f_act(I.W0 + b_0) |-----.\n",
    "      w_x_1  / l                   j     |      _________________\n",
    "        \\   /   \\_________________/      |     /                 \\\n",
    "         \\ /                           h0|____j                   l\n",
    "          \\                               ____| f_act(H.Wh + b_h) |------ output\n",
    "         / \\     _________________     h1|    l                   j\n",
    "    w_y_0   \\   /                 \\      |     \\_________________/\n",
    "      /      \\_j                   l     |\n",
    " ____/__w_y_1__| f_act(I.W1 + b_1) |-----'\n",
    "y              l                   j   w_h_1\n",
    "                \\_________________/\n",
    "           \n",
    "Where:\n",
    "x = input x\n",
    "y = input y\n",
    "b_0 = bias neuron 0\n",
    "b_1 = bias neuron 1\n",
    "b_h = bias output neuron\n",
    "f_act = activation function\n",
    "I = vector of inputs [x, y]\n",
    "H = vector of hidden activations [h0, h1]]\n",
    "W0 = vector of weights [w_x_0, w_y_0]\n",
    "W1 = vector of weights [w_x_1, w_y_1]\n",
    "Wh = vector of weights [w_h_0, w_h_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h0 = f\\_act(\\sum_{i=0}^{1}{(I_{i} * W0_{i}) + b\\_0})$$\n",
    "$$h1 = f\\_act(\\sum_{i=0}^{1}{(I_{i} * W1_{i}) + b\\_1})$$\n",
    "$$output = f\\_act(\\sum_{i=0}^{1}{(H_{i} * Wh_{i}) + b\\_h})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to plot the MLP output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = np.arange(-1.2, 1.2, 0.1)\n",
    "input_y = np.arange(-1.2, 1.2, 0.1)\n",
    "\n",
    "input_x_matrix, input_y_matrix = np.meshgrid(input_x, input_y)\n",
    "inputs_xy = np.concatenate((input_x_matrix.flatten()[:,np.newaxis], input_y_matrix.flatten()[:,np.newaxis]), axis=1)\n",
    "\n",
    "def plot_MLP(w_x_0, w_y_0, b_0, w_x_1, w_y_1, b_1, w_h_0, w_h_1, b_h, activation_function_index):\n",
    "    w_0 = np.array([w_x_0, w_y_0])\n",
    "    w_1 = np.array([w_x_1, w_y_1])\n",
    "    w_h = np.array([w_h_0, w_h_1])\n",
    "    \n",
    "    activation_function = activation_functions_dict.get(list(activation_functions_dict.keys())[activation_function_index])\n",
    "    \n",
    "    h_0 = perceptron(inputs_xy, w_0, b_0, activation_function)\n",
    "    h_1 = perceptron(inputs_xy, w_1, b_1, activation_function)\n",
    "    h = np.array([h_0, h_1]).T\n",
    "    \n",
    "    output_values = perceptron(h, w_h, b_h, activation_function)\n",
    "    output_matrix = np.reshape(output_values, input_x_matrix.shape)\n",
    "    \n",
    "    pl.figure(figsize=(8,6))\n",
    "    pl.imshow(np.flipud(output_matrix), interpolation='None', extent=(-1.2,1.2,-1.2,1.2), vmin=-1.0, vmax=1.0)\n",
    "    pl.xlabel('x')\n",
    "    pl.ylabel('y')\n",
    "    pl.colorbar()\n",
    "    pl.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_slider(name):\n",
    "    return widgets.FloatSlider(\n",
    "        value=0.5,\n",
    "        min=-2.0,\n",
    "        max=2.0,\n",
    "        step=0.01,\n",
    "        description=name,\n",
    "    )\n",
    "\n",
    "def create_controls():\n",
    "    controls = {name:create_slider(name) for name in ['w_x_0', 'w_y_0', 'b_0', 'w_x_1', 'w_y_1', 'b_1', 'w_h_0', 'w_h_1', 'b_h']}\n",
    "\n",
    "    controls['activation_function_index'] = widgets.Dropdown(\n",
    "        options={list(activation_functions_dict.keys())[i]:i for i in range(len(activation_functions_dict))},\n",
    "        value=1,\n",
    "        description='Activation function:',\n",
    "    )\n",
    "    return controls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the MLP output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c4a0ad1ab5468583f1d15306879fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>VBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "VBox(children=(HBox(children=(FloatSlider(value=0.5, description='w_x_0', max=2.0, min=-2.0, step=0.01), FloatSlider(value=0.5, description='w_y_0', max=2.0, min=-2.0, step=0.01), FloatSlider(value=0.5, description='b_0', max=2.0, min=-2.0, step=0.01))), HBox(children=(FloatSlider(value=0.5, description='w_x_1', max=2.0, min=-2.0, step=0.01), FloatSlider(value=0.5, description='w_y_1', max=2.0, min=-2.0, step=0.01), FloatSlider(value=0.5, description='b_1', max=2.0, min=-2.0, step=0.01))), HBox(children=(FloatSlider(value=0.5, description='w_h_0', max=2.0, min=-2.0, step=0.01), FloatSlider(value=0.5, description='w_h_1', max=2.0, min=-2.0, step=0.01), FloatSlider(value=0.5, description='b_h', max=2.0, min=-2.0, step=0.01))), Dropdown(description='Activation function:', index=1, options={'Linear': 0, 'Hyperbolic tangent': 1, 'Gaussian': 2, 'Sigmoid': 3}, value=1), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "controls = create_controls()\n",
    "\n",
    "# Creating the interactive widget with VBox layout\n",
    "w = widgets.interactive(plot_MLP, **controls)\n",
    "# Changing the format\n",
    "UI = VBox([HBox(children=list(w.children[0:3])),\n",
    "           HBox(children=list(w.children[3:6])),\n",
    "           HBox(children=list(w.children[6:9])),\n",
    "           w.children[-2],\n",
    "           w.children[-1]])\n",
    "\n",
    "display(UI)\n",
    "\n",
    "# Move the controls to see the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the sliders to change the values of the connection weights and biases, and observe the resulting changes in the MLP output\n",
    "- Change the activation function and observe the changes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlg-bachelor",
   "language": "python",
   "name": "mlg-bachelor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  },
  "widgets": {
   "state": {
    "639706f4b1114f73a07782e82a2f0f26": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "7c19e57a938644bca4e7ef98824106b5": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "84b7cb6dc4a04dc1b8c7bcc1c4362ea0": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "bc5bbb42d5144a80ba21bd5fc99342d5": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
