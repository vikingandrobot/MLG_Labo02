{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "Activation functions are essential to artificial neural networks. They are used to compute the output of artificial neurons and therefore, the output of the network.\n",
    "An activation function must be differentiable if a learning algorithm like backpropagation wants to be used to find the network parameters.\n",
    "\n",
    "This notebook shows some examples of activation functions, and how their shape change with respect to the weight of the connections between neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of some activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear\n",
    "$$output = neta$$\n",
    "\n",
    "Sigmoid\n",
    "$$output = \\frac {1}{1 + e^{-neta}}$$\n",
    "\n",
    "Hyperbolic tangent\n",
    "$$output = \\frac {e^{neta} - e^{-neta}}{e^{neta} + e^{-neta}}$$\n",
    "\n",
    "Gaussian\n",
    "$$output = e^{-neta^{2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(neta):\n",
    "    output = neta\n",
    "    d_output = np.ones(len(neta))\n",
    "    return (output, d_output)\n",
    "\n",
    "def sigmoid(neta):\n",
    "    output = 1 / (1 + np.exp(-neta))\n",
    "    d_output = output * (1 - output)\n",
    "    return (output, d_output)\n",
    "\n",
    "def htan(neta):\n",
    "    exp = np.exp(neta)\n",
    "    m_exp = np.exp(-neta)\n",
    "    output = (exp - m_exp ) / (exp + m_exp)\n",
    "    d_output = 1 - (output * output)\n",
    "    return (output, d_output)\n",
    "\n",
    "def gaussian(neta):\n",
    "    output = np.exp(-1 * neta * neta)\n",
    "    d_output = -2 * neta * output\n",
    "    return (output, d_output)\n",
    "\n",
    "activation_functions_dict = {'Linear': linear, 'Sigmoid': sigmoid, 'Hyperbolic tangent': htan, 'Gaussian':gaussian}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to plot the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values = np.arange(-10, 10, 0.01)\n",
    "\n",
    "def plot_activation_function(activation_function_index, weight):\n",
    "    neta = weight * input_values\n",
    "    \n",
    "    activation_function = activation_functions_dict.get(list(activation_functions_dict.keys())[activation_function_index])\n",
    "    output_value, d_output_value = activation_function(neta)\n",
    "    \n",
    "    pl.figure(figsize=(8,6))\n",
    "    pl.plot(input_values, output_value, label='output')\n",
    "    pl.plot(input_values, weight * d_output_value, c='r', label='first derivative')\n",
    "    pl.xlabel('Input value')\n",
    "    pl.ylabel('Output value')\n",
    "    pl.ylim(-1.1, 1.1)\n",
    "    pl.legend(loc=4)\n",
    "    pl.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_slider = widgets.FloatSlider(\n",
    "    value=1.0,\n",
    "    min=-2.0,\n",
    "    max=2.0,\n",
    "    step=0.01,\n",
    "    description='Weight',\n",
    ")\n",
    "activation_function_list = widgets.Dropdown(\n",
    "    options={list(activation_functions_dict.keys())[i]:i for i in range(len(activation_functions_dict))},\n",
    "    value=1,\n",
    "    description='Activation function',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea56bb459d345f998546eac00ad8f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(Dropdown(description='Activation function', index=1, options={'Gaussian': 0, 'Hyperbolic tangent': 1, 'Linear': 2, 'Sigmoid': 3}, value=1), FloatSlider(value=1.0, description='Weight', max=2.0, min=-2.0, step=0.01), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(plot_activation_function, activation_function_index=activation_function_list, weight=weight_slider);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observe the shape of the different activation functions proposed.\n",
    "\n",
    "- Observe the effects of modifying the weight. How the shape of the function changes? How the first derivative changes?\n",
    "\n",
    "- Implement the activation function of a rectified Linear Unit (ReLU)\n",
    "\n",
    "$$ f(x) = \\left \\{\n",
    "\\begin{array}{rcl}\n",
    "\t0 & \\mbox{for} & x < 0\\\\\n",
    "\tx & \\mbox{for} & x \\ge 0\n",
    "\\end{array} \\right.\n",
    "\\hspace{1cm}\n",
    "f'(x) = \\left \\{\n",
    "\\begin{array}{rcl}\n",
    "\t0 & \\mbox{for} & x < 0\\\\\n",
    "\t1 & \\mbox{for} & x \\ge 0\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "- Visualize the ReLu activation function using the tools given in this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {
    "ea3749fc36a04f93b2dee9a4a68cd815": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
